{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/Prakhar-porwal/local-voice-tts-clean/blob/main/notebooks/fast_tts_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Local Voice TTS - Turbo Mode (Google Colab)\n",
                "\n",
                "This notebook runs the **Local Voice TTS Backend** on a **Free Google Colab T4 GPU**.\n",
                "This is roughly **10x-20x faster** than the Hugging Face CPU Free Tier.\n",
                "\n",
                "### Instructions:\n",
                "1.  **Runtime** -> **Change runtime type** -> **T4 GPU** (should be default).\n",
                "2.  **Runtime** -> **Run all**.\n",
                "3.  Wait for the **Public URL** (ending in `ngrok-free.app` or similar) to appear at the bottom.\n",
                "4.  Copy that URL and use it in your Vercel Frontend!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Setup & Install\n",
                "# Clone the repository\n",
                "!git clone https://github.com/Prakhar-porwal/local-voice-tts-clean\n",
                "%cd local-voice-tts-clean\n",
                "\n",
                "# Install system dependencies\n",
                "!apt-get -y install espeak-ng libsndfile1\n",
                "\n",
                "# Install Python dependencies\n",
                "# 1. Remove torch (Colab has it)\n",
                "# 2. Remove TTS (we install from source to fix Python 3.12 compatibility)\n",
                "!sed -i '/torch/d' requirements.txt\n",
                "!sed -i '/TTS/d' requirements.txt\n",
                "!pip install -r requirements.txt\n",
                "!pip install pyngrok nest_asyncio uvicorn\n",
                "\n",
                "# Install TTS from source (bypasses PyPI version check for Py3.12)\n",
                "!pip install git+https://github.com/coqui-ai/TTS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Start the Turbo Backend\n",
                "import os\n",
                "import threading\n",
                "from pyngrok import ngrok\n",
                "import uvicorn\n",
                "import nest_asyncio\n",
                "\n",
                "nest_asyncio.apply()\n",
                "\n",
                "# --- SET YOUR NGROK TOKEN IF YOU HAVE ONE ---\n",
                "\n",
                "# ngrok.set_auth_token(\"YOUR_TOKEN_HERE\")\n",
                "\n",
                "# Set up the tunnel\n",
                "try:\n",
                "  public_url = ngrok.connect(7860).public_url\n",
                "except Exception as e:\n",
                "  print(f\"Ngrok error (you might need to restart runtime if 'already online'): {e}\")\n",
                "  # Fallback to listing tunnels if they exist\n",
                "  tunnels = ngrok.get_tunnels()\n",
                "  if tunnels:\n",
                "    public_url = tunnels[0].public_url\n",
                "\n",
                "print(\"\\n================================================================\")\n",
                "print(f\"ðŸš€ TURBO API URL: {public_url}\")\n",
                "print(\"================================================================\\n\")\n",
                "print(\"Copy the URL above and paste it into your App settings or .env!\")\n",
                "\n",
                "# Run the Server\n",
                "# We force CUDA (GPU) if available\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
                "uvicorn.run(\"main:app\", host=\"127.0.0.1\", port=7860)"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}