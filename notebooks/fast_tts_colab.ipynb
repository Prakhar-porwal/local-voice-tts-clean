{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prakhar-porwal/local-voice-tts-clean/blob/main/notebooks/fast_tts_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Local Voice TTS - Turbo Mode (Google Colab)\n",
        "\n",
        "This notebook runs the **Local Voice TTS Backend** on a **Free Google Colab T4 GPU**.\n",
        "This is roughly **10x-20x faster** than the Hugging Face CPU Free Tier.\n",
        "\n",
        "### Instructions:\n",
        "1.  **Runtime** -> **Change runtime type** -> **T4 GPU** (should be default).\n",
        "2.  **Runtime** -> **Disconnect and Delete Runtime** (If retrying).\n",
        "3.  Run **Cell 1** (Setup).\n",
        "4.  **Restart Session** (When prompted by Cell 1).\n",
        "5.  Run **Cell 2** (Start Server).\n",
        "6.  Copy the **TURBO API URL** into your Vercel App Settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1. Setup & Install (Correct Order)\n",
        "import os\n",
        "\n",
        "# 1. Clean previous runs\n",
        "if os.path.exists(\"local-voice-tts-clean\"):\n",
        "    !rm -rf local-voice-tts-clean\n",
        "\n",
        "# 2. Clone Repository\n",
        "!git clone https://github.com/Prakhar-porwal/local-voice-tts-clean\n",
        "%cd local-voice-tts-clean\n",
        "\n",
        "# 3. System Dependencies\n",
        "!apt-get -y install espeak-ng libsndfile1\n",
        "\n",
        "# 4. Install Build Tools first\n",
        "!pip install hatchling cython numpy==1.26.4\n",
        "\n",
        "# 5. Install TTS (THIS UPGRADES TRANSFORMERS, SO WE DO IT FIRST)\n",
        "print(\"Installing TTS...\")\n",
        "!pip install --no-build-isolation \"git+https://github.com/idiap/coqui-ai-TTS.git#egg=coqui-tts\"\n",
        "\n",
        "# 6. OVERRIDE with Correct Dependencies\n",
        "print(\"ðŸ”§ Forcing Correct Package Versions...\")\n",
        "!pip install --force-reinstall transformers==4.40.0 accelerate==0.29.0\n",
        "!pip install --force-reinstall \"numpy<2.0\" \"scipy\" setuptools wheel\n",
        "\n",
        "# 7. Install Server Deps\n",
        "!pip install fastapi \"uvicorn[standard]\" librosa soundfile pydub python-multipart pyngrok nest_asyncio\n",
        "\n",
        "print(\"\\nâœ… Setup Complete. Please RESTART RUNTIME & RUN CELL 2.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2. Start Turbo Backend\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import transformers\n",
        "\n",
        "print(f\"ðŸ§ Transformers Version: {transformers.__version__} (Should be 4.40.0)\")\n",
        "\n",
        "# --- 1. APPLY HOTFIX ---\n",
        "target_file = \"/usr/local/lib/python3.12/dist-packages/TTS/tts/layers/tortoise/autoregressive.py\"\n",
        "if os.path.exists(target_file):\n",
        "    with open(target_file, \"r\") as f:\n",
        "        content = f.read()\n",
        "    if \"isin_mps_friendly\" in content:\n",
        "        print(\"ðŸ”§ Patching TTS Library...\")\n",
        "        new_content = content.replace(\"from transformers.pytorch_utils import isin_mps_friendly as isin\", \"from torch import isin\")\n",
        "        with open(target_file, \"w\") as f:\n",
        "            f.write(new_content)\n",
        "\n",
        "# --- 2. PATCH main.py (GPU) ---\n",
        "main_file = \"/content/local-voice-tts-clean/main.py\"\n",
        "if os.path.exists(main_file):\n",
        "    print(\"ðŸ”§ Patching main.py for CUDA/GPU Support...\")\n",
        "    with open(main_file, \"r\") as f:\n",
        "        code = f.read()\n",
        "    code = code.replace('if torch.backends.mps.is_available():\\n    device = \"mps\"\\nelse:\\n    device = \"cpu\"', 'device = \"cuda\" if torch.cuda.is_available() else \"cpu\"')\n",
        "    code = code.replace('xtts_tts = TTS(XTTS_MODEL).to(\"cpu\")', 'xtts_tts = TTS(XTTS_MODEL).to(device)')\n",
        "    with open(main_file, \"w\") as f:\n",
        "        f.write(code)\n",
        "    print(\"âœ… GPU Patch Applied!\")\n",
        "\n",
        "# --- 3. START SERVER ---\n",
        "print(\"ðŸš€ Starting Server...\")\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# Auto-Accept License\n",
        "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
        "# Force GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Reset Ngrok\n",
        "ngrok.kill()\n",
        "# ðŸ”‘ SET AUTH TOKEN\n",
        "ngrok.set_auth_token(\"36jQzzt9HifzkJJKUwtGTOAt3nA_3hZTGahebR2E5xk4WDX7o\")\n",
        "nest_asyncio.apply()\n",
        "\n",
        "try:\n",
        "    print(\"â³ Connecting to Ngrok...\")\n",
        "    time.sleep(2)\n",
        "    public_url = ngrok.connect(7860).public_url\n",
        "    print(\"\\n================================================================\")\n",
        "    print(f\"ðŸš€ TURBO API URL: {public_url}\")\n",
        "    print(\"================================================================\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Ngrok Error: {e}\")\n",
        "    print(\"âš ï¸ IF THIS SAYS 'ERR_NGROK_108': Your account has too many active sessions.\")\n",
        "    print(\"   -> Go to dashboard.ngrok.com and kill old agents, OR use a new token.\")\n",
        "\n",
        "if os.path.exists(\"/content/local-voice-tts-clean\"):\n",
        "    os.chdir(\"/content/local-voice-tts-clean\")\n",
        "\n",
        "config = uvicorn.Config(\"main:app\", host=\"127.0.0.1\", port=7860)\n",
        "server = uvicorn.Server(config)\n",
        "import asyncio\n",
        "await server.serve()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}