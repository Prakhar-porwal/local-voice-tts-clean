{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/Prakhar-porwal/local-voice-tts-clean/blob/main/notebooks/fast_tts_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Local Voice TTS - Turbo Mode (Google Colab)\n",
                "\n",
                "This notebook runs the **Local Voice TTS Backend** on a **Free Google Colab T4 GPU**.\n",
                "This is roughly **10x-20x faster** than the Hugging Face CPU Free Tier.\n",
                "\n",
                "### Instructions:\n",
                "1.  **Runtime** -> **Change runtime type** -> **T4 GPU** (should be default).\n",
                "2.  **Runtime** -> **Run all**.\n",
                "3.  Wait for the **Public URL** (ending in `ngrok-free.app` or similar) to appear at the bottom.\n",
                "4.  Copy that URL and use it in your Vercel Frontend!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Setup & Install (Final Fix)\n",
                "import os\n",
                "\n",
                "# 1. Clean previous runs\n",
                "if os.path.exists(\"local-voice-tts-clean\"):\n",
                "    !rm -rf local-voice-tts-clean\n",
                "\n",
                "# 2. Clone Repository\n",
                "!git clone https://github.com/Prakhar-porwal/local-voice-tts-clean\n",
                "%cd local-voice-tts-clean\n",
                "\n",
                "# 3. System Dependencies\n",
                "!apt-get -y install espeak-ng libsndfile1\n",
                "\n",
                "# 4. Install Dependencies (Transformers 4.40.0 is CRITICAL for Colab)\n",
                "!pip install transformers==4.40.0\n",
                "!pip install \"numpy<2.0\" setuptools wheel\n",
                "\n",
                "# 5. Install Other Deps\n",
                "!pip install accelerate fastapi \"uvicorn[standard]\" librosa soundfile pydub python-multipart pyngrok nest_asyncio\n",
                "\n",
                "# 6. Install TTS from Source (Community Fork)\n",
                "print(\"Installing TTS...\")\n",
                "!pip install --no-build-isolation \"git+https://github.com/idiap/coqui-ai-TTS.git#egg=coqui-tts\"\n",
                "\n",
                "print(\"\\nâœ… Setup Complete. Please RUN CELL 2 now.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Start Turbo Backend (Auto-Fix & Run)\n",
                "import os\n",
                "import threading\n",
                "from pyngrok import ngrok\n",
                "import uvicorn\n",
                "import nest_asyncio\n",
                "\n",
                "# --- 1. AUTO-ACCEPT LICENSE TO PREVENT HANGING ---\n",
                "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
                "\n",
                "# --- 2. APPLY HOTFIX FOR 'isin_mps_friendly' ERROR ---\n",
                "target_file = \"/usr/local/lib/python3.12/dist-packages/TTS/tts/layers/tortoise/autoregressive.py\"\n",
                "if os.path.exists(target_file):\n",
                "    with open(target_file, \"r\") as f:\n",
                "        content = f.read()\n",
                "    # Replace missing transformer function with torch equivalent\n",
                "    if \"isin_mps_friendly\" in content:\n",
                "        print(\"Applying Hotfix to TTS library...\")\n",
                "        new_content = content.replace(\n",
                "            \"from transformers.pytorch_utils import isin_mps_friendly as isin\", \n",
                "            \"from torch import isin\"\n",
                "        )\n",
                "        with open(target_file, \"w\") as f:\n",
                "            f.write(new_content)\n",
                "        print(\"Hotfix applied! âœ…\")\n",
                "\n",
                "# --- 3. SETUP NGROK ---\n",
                "# FORCE KILL old tunnels to prevent \"Already Online\" error\n",
                "ngrok.kill()\n",
                "\n",
                "# SET YOUR TOKEN HERE\n",
                "ngrok_token = \"36jQzzt9HifzkJJKUwtGTOAt3nA_3hZTGahebR2E5xk4WDX7o\"\n",
                "ngrok.set_auth_token(ngrok_token)\n",
                "\n",
                "nest_asyncio.apply()\n",
                "\n",
                "try:\n",
                "    public_url = ngrok.connect(7860).public_url\n",
                "    print(\"\\n================================================================\")\n",
                "    print(f\"ðŸš€ TURBO API URL: {public_url}\")\n",
                "    print(\"================================================================\\n\")\n",
                "    print(\"Copy the URL above and paste it into your App settings!\")\n",
                "except Exception as e:\n",
                "    print(f\"Ngrok Error: {e}\")\n",
                "    # Try to get existing tunnel if connect failed\n",
                "    tunnels = ngrok.get_tunnels()\n",
                "    if tunnels:\n",
                "        print(f\"ðŸš€ EXISTING URL: {tunnels[0].public_url}\")\n",
                "\n",
                "# --- 4. START SERVER ---\n",
                "# Change directory to ensure we find main.py\n",
                "os.chdir(\"/content/local-voice-tts-clean\")\n",
                "print(\"Starting Server in: \" + os.getcwd())\n",
                "\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
                "config = uvicorn.Config(\"main:app\", host=\"127.0.0.1\", port=7860)\n",
                "server = uvicorn.Server(config)\n",
                "import asyncio\n",
                "await server.serve()"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}